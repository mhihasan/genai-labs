{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Next Word Prediction using LSTM and GRU",
   "id": "fe47a17d588ce44a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T03:04:44.414185Z",
     "start_time": "2025-08-12T03:04:44.408606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n"
   ],
   "id": "30c003a450aca660",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Data Collection",
   "id": "5a1d39860b13de8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T03:05:12.818858Z",
     "start_time": "2025-08-12T03:05:09.573648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataCollector:\n",
    "    \"\"\"\n",
    "    Handles collection and combination of text data from various sources.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str = \"data\"):\n",
    "        \"\"\"\n",
    "        Initialize the data collector.\n",
    "\n",
    "        Args:\n",
    "            data_dir: Directory to store downloaded data\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    def download_gutenberg_books(self, book_urls: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Download books from Project Gutenberg.\n",
    "\n",
    "        Args:\n",
    "            book_urls: List of URLs to download\n",
    "\n",
    "        Returns:\n",
    "            Combined text content\n",
    "        \"\"\"\n",
    "        combined_text = \"\"\n",
    "\n",
    "        for i, url in enumerate(book_urls):\n",
    "            try:\n",
    "                print(f\"Downloading book {i+1}/{len(book_urls)}...\")\n",
    "                response = requests.get(url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # Basic cleaning of Gutenberg headers/footers\n",
    "                text = response.text\n",
    "                start_marker = \"*** START OF THE PROJECT GUTENBERG\"\n",
    "                end_marker = \"*** END OF THE PROJECT GUTENBERG\"\n",
    "\n",
    "                start_idx = text.find(start_marker)\n",
    "                end_idx = text.find(end_marker)\n",
    "\n",
    "                if start_idx != -1 and end_idx != -1:\n",
    "                    text = text[start_idx:end_idx]\n",
    "\n",
    "                combined_text += text + \"\\n\\n\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return combined_text\n",
    "\n",
    "    def save_text_data(self, text: str, filename: str) -> None:\n",
    "        \"\"\"Save text data to file.\"\"\"\n",
    "        filepath = os.path.join(self.data_dir, filename)\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        print(f\"Saved text data to {filepath}\")\n",
    "\n",
    "    def load_text_data(self, filename: str) -> str:\n",
    "        \"\"\"Load text data from file.\"\"\"\n",
    "        filepath = os.path.join(self.data_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {filepath} not found.\")\n",
    "            return \"\"\n",
    "\n",
    "# Initialize data collector\n",
    "data_collector = DataCollector()\n",
    "\n",
    "# Download sample books from Project Gutenberg\n",
    "gutenberg_urls = [\n",
    "    \"https://www.gutenberg.org/files/1342/1342-0.txt\",  # Pride and Prejudice\n",
    "    \"https://www.gutenberg.org/files/11/11-0.txt\",      # Alice in Wonderland\n",
    "    \"https://www.gutenberg.org/files/74/74-0.txt\",      # The Adventures of Tom Sawyer\n",
    "    \"https://www.gutenberg.org/files/2701/2701-0.txt\",  # Moby Dick\n",
    "    \"https://www.gutenberg.org/files/1661/1661-0.txt\",  # The Adventures of Sherlock Holmes\n",
    "]\n",
    "\n",
    "# Download and combine text data\n",
    "print(\"Collecting training data...\")\n",
    "raw_text = data_collector.download_gutenberg_books(gutenberg_urls)\n",
    "data_collector.save_text_data(raw_text, \"combined_books.txt\")\n",
    "\n",
    "print(f\"Total characters collected: {len(raw_text):,}\")\n",
    "print(f\"First 500 characters:\\n{raw_text[:500]}\")\n"
   ],
   "id": "3031c1aaa6337f32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting training data...\n",
      "Downloading book 1/5...\n",
      "Downloading book 2/5...\n",
      "Downloading book 3/5...\n",
      "Downloading book 4/5...\n",
      "Downloading book 5/5...\n",
      "Saved text data to data/combined_books.txt\n",
      "Total characters collected: 3,095,941\n",
      "First 500 characters:\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 1342 ***\r\n",
      "                            [Illustration:\r\n",
      "\r\n",
      "                             GEORGE ALLEN\r\n",
      "                               PUBLISHER\r\n",
      "\r\n",
      "                        156 CHARING CROSS ROAD\r\n",
      "                                LONDON\r\n",
      "\r\n",
      "                             RUSKIN HOUSE\r\n",
      "                                   ]\r\n",
      "\r\n",
      "                            [Illustration:\r\n",
      "\r\n",
      "               _Reading Jane’s Letters._      _Chap 34._\r\n",
      "                                   ]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Data Preprocessing",
   "id": "87f6a53b49941f4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T03:05:18.489892Z",
     "start_time": "2025-08-12T03:05:16.894009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Handles text preprocessing for next-word prediction models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_vocab_size: int = 10000, sequence_length: int = 50):\n",
    "        \"\"\"\n",
    "        Initialize the text preprocessor.\n",
    "\n",
    "        Args:\n",
    "            max_vocab_size: Maximum vocabulary size\n",
    "            sequence_length: Length of input sequences\n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text data.\n",
    "\n",
    "        Args:\n",
    "            text: Raw text string\n",
    "\n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:\\'\"()-]', ' ', text)\n",
    "\n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def create_tokenizer(self, text: str) -> None:\n",
    "        \"\"\"\n",
    "        Create and fit tokenizer on text data.\n",
    "\n",
    "        Args:\n",
    "            text: Cleaned text data\n",
    "        \"\"\"\n",
    "        self.tokenizer = Tokenizer(\n",
    "            num_words=self.max_vocab_size,\n",
    "            oov_token=\"<OOV>\",\n",
    "            filters='',  # We already cleaned the text\n",
    "        )\n",
    "\n",
    "        # Split into sentences for better tokenization\n",
    "        sentences = text.split('.')\n",
    "        self.tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "        # Update vocab size (add 1 for OOV token)\n",
    "        self.vocab_size = min(len(self.tokenizer.word_index) + 1, self.max_vocab_size)\n",
    "\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Most common words: {list(self.tokenizer.word_index.keys())[:20]}\")\n",
    "\n",
    "    def create_sequences(self, text: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create input sequences and targets for training.\n",
    "\n",
    "        Args:\n",
    "            text: Cleaned text data\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (input_sequences, targets)\n",
    "        \"\"\"\n",
    "        # Convert text to sequences\n",
    "        sequences = self.tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "        # Create input-target pairs\n",
    "        input_sequences = []\n",
    "        targets = []\n",
    "\n",
    "        print(\"Creating training sequences...\")\n",
    "        for i in tqdm(range(self.sequence_length, len(sequences))):\n",
    "            input_seq = sequences[i-self.sequence_length:i]\n",
    "            target = sequences[i]\n",
    "\n",
    "            input_sequences.append(input_seq)\n",
    "            targets.append(target)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(input_sequences)\n",
    "        y = np.array(targets)\n",
    "\n",
    "        print(f\"Created {len(X):,} training sequences\")\n",
    "        print(f\"Input shape: {X.shape}\")\n",
    "        print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def save_tokenizer(self, filepath: str) -> None:\n",
    "        \"\"\"Save tokenizer to file.\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'tokenizer': self.tokenizer,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'sequence_length': self.sequence_length,\n",
    "                'max_vocab_size': self.max_vocab_size\n",
    "            }, f)\n",
    "        print(f\"Tokenizer saved to {filepath}\")\n",
    "\n",
    "    def load_tokenizer(self, filepath: str) -> None:\n",
    "        \"\"\"Load tokenizer from file.\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.tokenizer = data['tokenizer']\n",
    "            self.vocab_size = data['vocab_size']\n",
    "            self.sequence_length = data['sequence_length']\n",
    "            self.max_vocab_size = data['max_vocab_size']\n",
    "        print(f\"Tokenizer loaded from {filepath}\")\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(max_vocab_size=15000, sequence_length=40)\n",
    "\n",
    "# Load and preprocess text data\n",
    "raw_text = data_collector.load_text_data(\"combined_books.txt\")\n",
    "if not raw_text:\n",
    "    print(\"No data found. Please run the data collection section first.\")\n",
    "else:\n",
    "    # Clean text\n",
    "    print(\"Cleaning text...\")\n",
    "    clean_text = preprocessor.clean_text(raw_text)\n",
    "\n",
    "    # Create tokenizer\n",
    "    print(\"Creating tokenizer...\")\n",
    "    preprocessor.create_tokenizer(clean_text)\n",
    "\n",
    "    # Create training sequences\n",
    "    print(\"Creating training sequences...\")\n",
    "    X, y = preprocessor.create_sequences(clean_text)\n",
    "\n",
    "    # Save tokenizer\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    preprocessor.save_tokenizer(\"models/tokenizer.pkl\")\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Training targets shape: {y_train.shape}\")\n",
    "print(f\"Validation targets shape: {y_val.shape}\")\n"
   ],
   "id": "49c0e8672e70e7b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text...\n",
      "Creating tokenizer...\n",
      "Vocabulary size: 15000\n",
      "Most common words: ['<OOV>', 'the', 'and', 'of', 'to', 'a', 'in', 'i', 'that', 'it', 'was', 'he', 'his', 'you', 'as', 'with', 'but', 'for', 'is', 's']\n",
      "Creating training sequences...\n",
      "Creating training sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 551204/551204 [00:00<00:00, 1078196.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 551,204 training sequences\n",
      "Input shape: (551204, 40)\n",
      "Target shape: (551204,)\n",
      "Tokenizer saved to models/tokenizer.pkl\n",
      "Training data shape: (440963, 40)\n",
      "Validation data shape: (110241, 40)\n",
      "Training targets shape: (440963,)\n",
      "Validation targets shape: (110241,)\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Model Architecture and Training",
   "id": "cf80883167d2f0c5"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-12T03:05:35.425081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NextWordPredictor:\n",
    "    \"\"\"\n",
    "    Next-word prediction model using LSTM or GRU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, sequence_length: int, embedding_dim: int = 128):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            sequence_length: Length of input sequences\n",
    "            embedding_dim: Dimensionality of embeddings\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "\n",
    "    def build_lstm_model(self, lstm_units: int = 256, dropout_rate: float = 0.3) -> None:\n",
    "        \"\"\"\n",
    "        Build LSTM-based model.\n",
    "\n",
    "        Args:\n",
    "            lstm_units: Number of LSTM units\n",
    "            dropout_rate: Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        self.model = Sequential([\n",
    "            Embedding(\n",
    "                input_dim=self.vocab_size,\n",
    "                output_dim=self.embedding_dim,\n",
    "                input_length=self.sequence_length,\n",
    "                name='embedding'\n",
    "            ),\n",
    "            LSTM(lstm_units, return_sequences=True, name='lstm_1'),\n",
    "            Dropout(dropout_rate, name='dropout_1'),\n",
    "            LSTM(lstm_units//2, name='lstm_2'),\n",
    "            Dropout(dropout_rate, name='dropout_2'),\n",
    "            Dense(self.vocab_size, activation='softmax', name='output')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        print(\"LSTM Model Architecture:\")\n",
    "        self.model.summary()\n",
    "\n",
    "    def build_gru_model(self, gru_units: int = 256, dropout_rate: float = 0.3) -> None:\n",
    "        \"\"\"\n",
    "        Build GRU-based model.\n",
    "\n",
    "        Args:\n",
    "            gru_units: Number of GRU units\n",
    "            dropout_rate: Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        self.model = Sequential([\n",
    "            Embedding(\n",
    "                input_dim=self.vocab_size,\n",
    "                output_dim=self.embedding_dim,\n",
    "                input_length=self.sequence_length,\n",
    "                name='embedding'\n",
    "            ),\n",
    "            GRU(gru_units, return_sequences=True, name='gru_1'),\n",
    "            Dropout(dropout_rate, name='dropout_1'),\n",
    "            GRU(gru_units//2, name='gru_2'),\n",
    "            Dropout(dropout_rate, name='dropout_2'),\n",
    "            Dense(self.vocab_size, activation='softmax', name='output')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        print(\"GRU Model Architecture:\")\n",
    "        self.model.summary()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray,\n",
    "              X_val: np.ndarray, y_val: np.ndarray,\n",
    "              model_name: str, epochs: int = 50, batch_size: int = 128) -> None:\n",
    "        \"\"\"\n",
    "        Train the model with callbacks.\n",
    "\n",
    "        Args:\n",
    "            X_train: Training input sequences\n",
    "            y_train: Training targets\n",
    "            X_val: Validation input sequences\n",
    "            y_val: Validation targets\n",
    "            model_name: Name for saving model checkpoints\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Training batch size\n",
    "        \"\"\"\n",
    "        # Create models directory\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(\n",
    "                filepath=f\"models/{model_name}_best.h5\",\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                verbose=1\n",
    "            ),\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        print(f\"Training {model_name} model...\")\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Save final model\n",
    "        self.model.save(f\"models/{model_name}_final.h5\")\n",
    "        print(f\"Model saved as models/{model_name}_final.h5\")\n",
    "\n",
    "    def plot_training_history(self) -> None:\n",
    "        \"\"\"Plot training history.\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"No training history available.\")\n",
    "            return\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        # Plot loss\n",
    "        ax1.plot(self.history.history['loss'], label='Training Loss')\n",
    "        ax1.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        ax1.set_title('Model Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Plot accuracy\n",
    "        ax2.plot(self.history.history['accuracy'], label='Training Accuracy')\n",
    "        ax2.plot(self.history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        ax2.set_title('Model Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ## When to Use LSTM vs GRU in Production\n",
    "\n",
    "print(\"\"\"\n",
    "## LSTM vs GRU: When to Use Each in Production\n",
    "\n",
    "### LSTM (Long Short-Term Memory)\n",
    "**Use LSTM when:**\n",
    "- You have complex, long sequences with intricate dependencies\n",
    "- Memory requirements are less of a concern\n",
    "- You need the most expressive model (has forget gate, input gate, output gate)\n",
    "- Working with tasks requiring fine-grained control over information flow\n",
    "- You have sufficient computational resources\n",
    "\n",
    "**Advantages:**\n",
    "- More expressive with separate forget and input gates\n",
    "- Better at capturing complex patterns in long sequences\n",
    "- More control over information flow\n",
    "\n",
    "**Disadvantages:**\n",
    "- More parameters (slower training and inference)\n",
    "- Higher memory requirements\n",
    "- More prone to overfitting on smaller datasets\n",
    "\n",
    "### GRU (Gated Recurrent Unit)\n",
    "**Use GRU when:**\n",
    "- You need faster training and inference\n",
    "- Working with limited computational resources\n",
    "- Dealing with shorter to medium-length sequences\n",
    "- Want to reduce overfitting risk\n",
    "- Performance is similar to LSTM but with fewer parameters\n",
    "\n",
    "**Advantages:**\n",
    "- Fewer parameters (faster training and inference)\n",
    "- Lower memory requirements\n",
    "- Less prone to overfitting\n",
    "- Simpler architecture, easier to tune\n",
    "\n",
    "**Disadvantages:**\n",
    "- Less expressive than LSTM\n",
    "- May not capture very complex, long-term dependencies as well\n",
    "\n",
    "### Production Decision Guidelines:\n",
    "1. **Start with GRU** for most applications (simpler, faster)\n",
    "2. **Switch to LSTM** if you need better performance on complex sequences\n",
    "3. **Consider computational constraints** in your production environment\n",
    "4. **A/B test both** if performance is critical\n",
    "\"\"\")\n",
    "\n",
    "# Train LSTM Model\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"TRAINING LSTM MODEL\")\n",
    "# print(\"=\"*50)\n",
    "\n",
    "# lstm_predictor = NextWordPredictor(\n",
    "#     vocab_size=preprocessor.vocab_size,\n",
    "#     sequence_length=preprocessor.sequence_length,\n",
    "#     embedding_dim=128\n",
    "# )\n",
    "#\n",
    "# lstm_predictor.build_lstm_model(lstm_units=256, dropout_rate=0.3)\n",
    "# lstm_predictor.train(X_train, y_train, X_val, y_val,\n",
    "#                     model_name=\"lstm_next_word\", epochs=30, batch_size=64)\n",
    "#\n",
    "# # Plot LSTM training history\n",
    "# lstm_predictor.plot_training_history()\n",
    "\n",
    "# Train GRU Model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING GRU MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "gru_predictor = NextWordPredictor(\n",
    "    vocab_size=preprocessor.vocab_size,\n",
    "    sequence_length=preprocessor.sequence_length,\n",
    "    embedding_dim=128\n",
    ")\n",
    "\n",
    "gru_predictor.build_gru_model(gru_units=256, dropout_rate=0.3)\n",
    "gru_predictor.train(X_train, y_train, X_val, y_val,\n",
    "                   model_name=\"gru_next_word\", epochs=30, batch_size=64)\n",
    "\n",
    "# Plot GRU training history\n",
    "gru_predictor.plot_training_history()\n"
   ],
   "id": "2a1e19e620e5c094",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## LSTM vs GRU: When to Use Each in Production\n",
      "\n",
      "### LSTM (Long Short-Term Memory)\n",
      "**Use LSTM when:**\n",
      "- You have complex, long sequences with intricate dependencies\n",
      "- Memory requirements are less of a concern\n",
      "- You need the most expressive model (has forget gate, input gate, output gate)\n",
      "- Working with tasks requiring fine-grained control over information flow\n",
      "- You have sufficient computational resources\n",
      "\n",
      "**Advantages:**\n",
      "- More expressive with separate forget and input gates\n",
      "- Better at capturing complex patterns in long sequences\n",
      "- More control over information flow\n",
      "\n",
      "**Disadvantages:**\n",
      "- More parameters (slower training and inference)\n",
      "- Higher memory requirements\n",
      "- More prone to overfitting on smaller datasets\n",
      "\n",
      "### GRU (Gated Recurrent Unit)\n",
      "**Use GRU when:**\n",
      "- You need faster training and inference\n",
      "- Working with limited computational resources\n",
      "- Dealing with shorter to medium-length sequences\n",
      "- Want to reduce overfitting risk\n",
      "- Performance is similar to LSTM but with fewer parameters\n",
      "\n",
      "**Advantages:**\n",
      "- Fewer parameters (faster training and inference)\n",
      "- Lower memory requirements\n",
      "- Less prone to overfitting\n",
      "- Simpler architecture, easier to tune\n",
      "\n",
      "**Disadvantages:**\n",
      "- Less expressive than LSTM\n",
      "- May not capture very complex, long-term dependencies as well\n",
      "\n",
      "### Production Decision Guidelines:\n",
      "1. **Start with GRU** for most applications (simpler, faster)\n",
      "2. **Switch to LSTM** if you need better performance on complex sequences\n",
      "3. **Consider computational constraints** in your production environment\n",
      "4. **A/B test both** if performance is critical\n",
      "\n",
      "\n",
      "==================================================\n",
      "TRAINING GRU MODEL\n",
      "==================================================\n",
      "GRU Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_7\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001B[38;5;33mEmbedding\u001B[0m)           │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001B[38;5;33mGRU\u001B[0m)                     │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             │ ?                      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_2 (\u001B[38;5;33mGRU\u001B[0m)                     │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001B[38;5;33mDropout\u001B[0m)             │ ?                      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001B[38;5;33mDense\u001B[0m)                  │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training gru_next_word model...\n",
      "Epoch 1/30\n",
      "\u001B[1m6806/6891\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m7s\u001B[0m 91ms/step - accuracy: 0.1017 - loss: 6.5999"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Model Prediction",
   "id": "90f4a0aba08cda34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T03:04:30.952702Z",
     "start_time": "2025-08-12T03:01:16.896734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NextWordInference:\n",
    "    \"\"\"\n",
    "    Handles inference for next-word prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, tokenizer_path: str):\n",
    "        \"\"\"\n",
    "        Initialize inference engine.\n",
    "\n",
    "        Args:\n",
    "            model_path: Path to trained model\n",
    "            tokenizer_path: Path to tokenizer\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.sequence_length = None\n",
    "        self.vocab_size = None\n",
    "\n",
    "    def load_model_and_tokenizer(self) -> None:\n",
    "        \"\"\"Load trained model and tokenizer.\"\"\"\n",
    "        try:\n",
    "            # Load model\n",
    "            self.model = load_model(self.model_path)\n",
    "            print(f\"Model loaded from {self.model_path}\")\n",
    "\n",
    "            # Load tokenizer\n",
    "            with open(self.tokenizer_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                self.tokenizer = data['tokenizer']\n",
    "                self.sequence_length = data['sequence_length']\n",
    "                self.vocab_size = data['vocab_size']\n",
    "\n",
    "            print(f\"Tokenizer loaded from {self.tokenizer_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model/tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_input(self, input_text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess input text for prediction.\n",
    "\n",
    "        Args:\n",
    "            input_text: Input text string\n",
    "\n",
    "        Returns:\n",
    "            Processed sequence array\n",
    "        \"\"\"\n",
    "        # Clean and tokenize input\n",
    "        input_text = input_text.lower().strip()\n",
    "        sequence = self.tokenizer.texts_to_sequences([input_text])[0]\n",
    "\n",
    "        # Pad or truncate to required length\n",
    "        if len(sequence) > self.sequence_length:\n",
    "            sequence = sequence[-self.sequence_length:]\n",
    "        else:\n",
    "            sequence = [0] * (self.sequence_length - len(sequence)) + sequence\n",
    "\n",
    "        return np.array([sequence])\n",
    "\n",
    "    def predict_next_word(self, input_text: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Predict next word(s) given input text.\n",
    "\n",
    "        Args:\n",
    "            input_text: Input text string\n",
    "            top_k: Number of top predictions to return\n",
    "\n",
    "        Returns:\n",
    "            List of (word, probability) tuples\n",
    "        \"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            self.load_model_and_tokenizer()\n",
    "\n",
    "        # Preprocess input\n",
    "        input_sequence = self.preprocess_input(input_text)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = self.model.predict(input_sequence, verbose=0)[0]\n",
    "\n",
    "        # Get top k predictions\n",
    "        top_indices = np.argsort(predictions)[-top_k:][::-1]\n",
    "\n",
    "        # Convert indices to words\n",
    "        word_index = self.tokenizer.word_index\n",
    "        index_word = {v: k for k, v in word_index.items()}\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if idx in index_word:\n",
    "                word = index_word[idx]\n",
    "                probability = predictions[idx]\n",
    "                results.append((word, float(probability)))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def predict_single_word(self, input_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predict single next word.\n",
    "\n",
    "        Args:\n",
    "            input_text: Input text string\n",
    "\n",
    "        Returns:\n",
    "            Predicted word\n",
    "        \"\"\"\n",
    "        predictions = self.predict_next_word(input_text, top_k=1)\n",
    "        return predictions[0][0] if predictions else \"<unknown>\"\n",
    "\n",
    "# Test the inference functions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING INFERENCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test with LSTM model\n",
    "lstm_inference = NextWordInference(\n",
    "    model_path=\"models/lstm_next_word_best.h5\",\n",
    "    tokenizer_path=\"models/tokenizer.pkl\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    lstm_inference.load_model_and_tokenizer()\n",
    "\n",
    "    # Test sentences\n",
    "    test_sentences = [\n",
    "        \"the quick brown fox\",\n",
    "        \"once upon a time\",\n",
    "        \"it was a dark and stormy\",\n",
    "        \"to be or not to\",\n",
    "        \"i think therefore i\"\n",
    "    ]\n",
    "\n",
    "    print(\"LSTM Predictions:\")\n",
    "    print(\"-\" * 40)\n",
    "    for sentence in test_sentences:\n",
    "        predictions = lstm_inference.predict_next_word(sentence, top_k=3)\n",
    "        print(f\"Input: '{sentence}'\")\n",
    "        for i, (word, prob) in enumerate(predictions, 1):\n",
    "            print(f\"  {i}. {word} ({prob:.3f})\")\n",
    "        print()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in LSTM inference: {e}\")\n",
    "\n",
    "# Test with GRU model\n",
    "gru_inference = NextWordInference(\n",
    "    model_path=\"models/gru_next_word_best.h5\",\n",
    "    tokenizer_path=\"models/tokenizer.pkl\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    gru_inference.load_model_and_tokenizer()\n",
    "\n",
    "    print(\"GRU Predictions:\")\n",
    "    print(\"-\" * 40)\n",
    "    for sentence in test_sentences:\n",
    "        predictions = gru_inference.predict_next_word(sentence, top_k=3)\n",
    "        print(f\"Input: '{sentence}'\")\n",
    "        for i, (word, prob) in enumerate(predictions, 1):\n",
    "            print(f\"  {i}. {word} ({prob:.3f})\")\n",
    "        print()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in GRU inference: {e}\")\n"
   ],
   "id": "a645203be6f1f2ca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING INFERENCE\n",
      "==================================================\n",
      "Model loaded from models/lstm_next_word_best.h5\n",
      "Tokenizer loaded from models/tokenizer.pkl\n",
      "LSTM Predictions:\n",
      "----------------------------------------\n",
      "Input: 'the quick brown fox'\n",
      "  1. the (0.055)\n",
      "  2. i (0.052)\n",
      "  3. he (0.047)\n",
      "\n",
      "Input: 'once upon a time'\n",
      "  1. of (0.143)\n",
      "  2. and (0.086)\n",
      "  3. to (0.053)\n",
      "\n",
      "Input: 'it was a dark and stormy'\n",
      "  1. i (0.075)\n",
      "  2. he (0.058)\n",
      "  3. she (0.051)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'to be or not to'\n",
      "  1. be (0.042)\n",
      "  2. not (0.037)\n",
      "  3. <OOV> (0.034)\n",
      "\n",
      "Input: 'i think therefore i'\n",
      "  1. be (0.077)\n",
      "  2. not (0.055)\n",
      "  3. <OOV> (0.051)\n",
      "\n",
      "Model loaded from models/gru_next_word_best.h5\n",
      "Tokenizer loaded from models/tokenizer.pkl\n",
      "GRU Predictions:\n",
      "----------------------------------------\n",
      "Input: 'the quick brown fox'\n",
      "  1. to (0.067)\n",
      "  2. and (0.061)\n",
      "  3. of (0.057)\n",
      "\n",
      "Input: 'once upon a time'\n",
      "  1. to (0.066)\n",
      "  2. of (0.064)\n",
      "  3. and (0.064)\n",
      "\n",
      "Input: 'it was a dark and stormy'\n",
      "  1. to (0.067)\n",
      "  2. and (0.065)\n",
      "  3. of (0.065)\n",
      "\n",
      "Input: 'to be or not to'\n",
      "  1. <OOV> (0.040)\n",
      "  2. the (0.023)\n",
      "  3. be (0.016)\n",
      "\n",
      "Input: 'i think therefore i'\n",
      "  1. <OOV> (0.040)\n",
      "  2. the (0.023)\n",
      "  3. be (0.017)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Model Evaluation",
   "id": "5daeebafcec4b88a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T03:04:30.953090Z",
     "start_time": "2025-08-12T03:02:36.045046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_models() -> None:\n",
    "    \"\"\"Evaluate both models on validation set.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Evaluate LSTM\n",
    "    try:\n",
    "        lstm_model = load_model(\"models/lstm_next_word_best.h5\")\n",
    "        lstm_loss, lstm_acc = lstm_model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"LSTM - Validation Loss: {lstm_loss:.4f}, Accuracy: {lstm_acc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating LSTM: {e}\")\n",
    "\n",
    "    # Evaluate GRU\n",
    "    try:\n",
    "        gru_model = load_model(\"models/gru_next_word_best.h5\")\n",
    "        gru_loss, gru_acc = gru_model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"GRU - Validation Loss: {gru_loss:.4f}, Accuracy: {gru_acc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating GRU: {e}\")\n",
    "\n",
    "evaluate_models()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Files created:\")\n",
    "print(\"- models/lstm_next_word_best.h5 (Best LSTM model)\")\n",
    "print(\"- models/gru_next_word_best.h5 (Best GRU model)\")\n",
    "print(\"- models/tokenizer.pkl (Tokenizer and preprocessing config)\")"
   ],
   "id": "ab9413e7ebc08dcc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODEL EVALUATION\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM - Validation Loss: 6.1113, Accuracy: 0.1007\n",
      "GRU - Validation Loss: 6.4292, Accuracy: 0.0586\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETE!\n",
      "==================================================\n",
      "Files created:\n",
      "- models/lstm_next_word_best.h5 (Best LSTM model)\n",
      "- models/gru_next_word_best.h5 (Best GRU model)\n",
      "- models/tokenizer.pkl (Tokenizer and preprocessing config)\n"
     ]
    }
   ],
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
